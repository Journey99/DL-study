# Multimodal AI ê¸°ì´ˆ

## 1. Unimodal vs Multimodal

### Unimodal AI (ë‹¨ì¼ ëª¨ë‹¬ë¦¬í‹°)
í•˜ë‚˜ì˜ ë°ì´í„° íƒ€ìž…ë§Œ ì²˜ë¦¬:

- Vision-only: ì´ë¯¸ì§€ ë¶„ë¥˜, ê°ì²´ íƒì§€ (ResNet, YOLO)
- Language-only: í…ìŠ¤íŠ¸ ìƒì„±, ë²ˆì—­ (GPT, BERT)
- Audio-only: ìŒì„± ì¸ì‹, ìŒì•… ìƒì„±

### Multimodal AI (ë‹¤ì¤‘ ëª¨ë‹¬ë¦¬í‹°)
ì—¬ëŸ¬ ë°ì´í„° íƒ€ìž…ì„ ë™ì‹œì— ì´í•´í•˜ê³  ì—°ê²°:

- Vision + Language: ì´ë¯¸ì§€ ìº¡ì…”ë‹, VQA
- Audio + Language: ìŒì„± ì–´ì‹œìŠ¤í„´íŠ¸
- Vision + Audio + Language: ë¹„ë””ì˜¤ ì´í•´

--- 

## 2. Multimodal Learningì˜ í•µì‹¬ ê°œë…

### 2.1 Modality (ëª¨ë‹¬ë¦¬í‹°)
ë°ì´í„°ì˜ "ì¢…ë¥˜"ë¥¼ ì˜ë¯¸

- visual modality : ì´ë¯¸ì§€, ë¹„ë””ì˜¤ (2D/3D í”½ì…€)
- textual modality : ë‹¨ì–´, ë¬¸ìž¥ (discrete tokens)
- audio modality : ìŒì„±, ì†Œë¦¬ (1D waveform)

ê° modalityëŠ” ë‹¤ë¥¸ êµ¬ì¡°ë¥¼ ê°€ì§
- ì´ë¯¸ì§€ : (H, W, C) í–‰ë ¬, ê³µê°„ì  êµ¬ì¡°
- í…ìŠ¤íŠ¸ : (Seq_len,) ì‹œí€€ìŠ¤, ìˆœì°¨ì  êµ¬ì¡°
- ì´ì§ˆì ì´ë¼ì„œ ì§ì ‘ ë¹„êµ ë¶ˆê°€

### 2.2 Representation (í‘œí˜„)
ê° modalityë¥¼ ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜

- Vision representation
    - image â†’ CNN/ViT â†’ [1024-dim vector]

- Language representation  
  - text â†’ BERT/GPT â†’ [768-dim vector]


**ë¬¸ì œ**: ì°¨ì›ë„ ë‹¤ë¥´ê³ , ì˜ë¯¸ ê³µê°„ë„ ë‹¤ë¦„
- ì´ë¯¸ì§€ ë²¡í„°ì˜ dim 100 â‰  í…ìŠ¤íŠ¸ ë²¡í„°ì˜ dim 100


### 2.3 Alignment (ì •ë ¬)
ì„œë¡œ ë‹¤ë¥¸ modalityë¥¼ **ê°™ì€ ì˜ë¯¸ ê³µê°„**ìœ¼ë¡œ ë§¤í•‘:

```
Image: [cat photo] â†’ [v1, v2, ..., v512]
                           â†“ alignment
Text:  "a cat"    â†’ [t1, t2, ..., t512]
```

ëª©í‘œ : ê°™ì€ ì˜ë¯¸ë©´ ë²¡í„°ê°€ ê°€ê¹Œì›Œì•¼ í•¨
- cosine_similarity(cat_image, "a cat") > 0.8
- cosine_similarity(cat_image, "a dog") < 0.3

#### Alignment ë°©ë²•
1. contrastive learning (ëŒ€ì¡° í•™ìŠµ)
- ê°™ì€ ê²ƒë¼ë¦¬ëŠ” ê°€ê¹ê²Œ, ë‹¤ë¥¸ ê²ƒë¼ë¦¬ëŠ” ë©€ê²Œ
2. cross-attention
- í•œ Modalityê°€ ë‹¤ë¥¸ modalityë¥¼ ì°¸ì¡°
- BLIP-2 ì˜ Q-Formerê°€ ì´ ë°©ì‹
3. simple projection
- Linear layerë¡œ ì°¨ì›ë§Œ ë§žì¶¤
- LLaVAê°€ ì´ ë°©ì‹ (ê°€ìž¥ ë‹¨ìˆœ)

---

## 3. Cross-modal Alignmentê°€ í•„ìš”í•œ ì´ìœ 

ì‹œë‚˜ë¦¬ì˜¤ : ì´ë¯¸ì§€ ìº¡ì…”ë‹

### Alignment ì—†ì´ (naive ë°©ë²•)
```
# 1. ì´ë¯¸ì§€ ë¶„ë¥˜
image â†’ ResNet â†’ "cat" (label)

# 2. í…ìŠ¤íŠ¸ ìƒì„±
"cat" â†’ GPT â†’ "A cat is sleeping on the couch"
```

ë¬¸ì œì 
- ì‹¤ì œ ì´ë¯¸ì§€ì—” ì†ŒíŒŒê°€ ì—†ëŠ”ë° hallucination ë°œìƒ
- labelë§Œìœ¼ë¡œëŠ” ë””í…Œì¼ ì „ë‹¬ ì•ˆë¨ (ìƒ‰ê¹”, ìžì„¸, ë°°ê²½ ë“±)
- ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ì§„ì§œ ì—°ê²° ëœê²Œ ì•„ë‹˜

### Alignment ìžˆìœ¼ë©´
```
# 1. Image â†’ visual features (spatial information ìœ ì§€)
image â†’ ViT â†’ [196ê°œ patch features]  # 14x14 grid

# 2. Visual featuresë¥¼ language spaceë¡œ projection
visual_features â†’ Projector â†’ [196ê°œ language-aligned features]

# 3. LLMì´ visual featuresë¥¼ "ë³´ë©´ì„œ" ìƒì„±
LLM("Describe: " + visual_features) 
    â†’ "An orange tabby cat is sleeping on a gray couch near a window"
```

visual featuresê°€ LLMì˜ embedding spaceì™€ alignedë˜ì–´ ìžˆì–´ì„œ
- llmì´ ì´ë¯¸ì§€ì˜ êµ¬ì²´ì  ë‚´ìš©ì„ ì´í•´
- spatial information í™œìš© ê°€ëŠ¥
- hallucination ê°ì†Œ

---

## 4. VLMì˜ ë™ìž‘ ì›ë¦¬ (ê°„ë‹¨í•œ ì˜ˆì‹œ)

### ì „ì²´ íë¦„
```
Input: 
  - Image: [ê³ ì–‘ì´ ì‚¬ì§„]
  - Text: "ì´ ë™ë¬¼ì€ ë¬´ì—‡ì¸ê°€ìš”?"

Step 1: Vision Encoding
  Image â†’ ViT â†’ Visual Tokens [v1, v2, ..., v196]
  # ê° í† í°ì€ ì´ë¯¸ì§€ì˜ í•œ patchë¥¼ í‘œí˜„

Step 2: Projection (Alignment)
  Visual Tokens â†’ Linear Layer â†’ [h1, h2, ..., h196]
  # h_iëŠ” LLMì˜ embedding spaceì— ìžˆìŒ

Step 3: Token Fusion
  Combined = [BOS] + [h1, ..., h196] + ["ì´", "ë™ë¬¼", "ì€", ...] + [EOS]
  # Visual tokensì™€ text tokensë¥¼ í•˜ë‚˜ë¡œ í•©ì¹¨

Step 4: LLM Decoding
  LLM(Combined) â†’ "ì´ê²ƒì€ ê³ ì–‘ì´ìž…ë‹ˆë‹¤. ì£¼í™©ìƒ‰ê³¼ í°ìƒ‰ í„¸ì„ ê°€ì§„ 
                   íƒœë¹„ ê³ ì–‘ì´ë¡œ ë³´ìž…ë‹ˆë‹¤."

```

#### ì™œ ì´ë ‡ê²Œ ìž‘ë™í•˜ë‚˜?
í•µì‹¬ ì•„ì´ë””ì–´: LLMì€ ì´ë¯¸ ì—„ì²­ë‚œ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥ì´ ìžˆìŒ

- GPT, LLaMA ê°™ì€ ëª¨ë¸ì€ ìˆ˜ì¡° ê°œì˜ í…ìŠ¤íŠ¸ë¡œ í•™ìŠµë¨
- "ê³ ì–‘ì´", "ì£¼í™©ìƒ‰", "í„¸" ê°™ì€ ê°œë…ì„ ì´ë¯¸ ì•Œê³  ìžˆìŒ

í•˜ì§€ë§Œ: ì´ë¯¸ì§€ëŠ” ë³¸ ì ì´ ì—†ìŒ

- ì´ë¯¸ì§€ í”½ì…€ì„ ì§ì ‘ ì£¼ë©´ ì´í•´ ëª»í•¨
- "í…ìŠ¤íŠ¸ì²˜ëŸ¼ ë³´ì´ê²Œ" ë³€í™˜ì´ í•„ìš” â†’ Projection!

---

## 5. Contrastive Learning ìƒì„¸ ì„¤ëª…
CLIP ê°™ì€ ëª¨ë¸ì˜ í•µì‹¬ í•™ìŠµ ë°©ë²•ìœ¼ë¡œ ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” "ê°™ì€ ì˜ë¯¸ëŠ” ê°€ê¹ê²Œ, ë‹¤ë¥¸ ì˜ë¯¸ëŠ” ë©€ê²Œ"

### í•™ìŠµ ê³¼ì •
```python
# Batch ë°ì´í„°
images = [img1, img2, img3, img4]  # 4ê°œ ì´ë¯¸ì§€
texts = ["a dog", "a cat", "a car", "a flower"]  # ëŒ€ì‘í•˜ëŠ” í…ìŠ¤íŠ¸

# Encoding
image_features = vision_encoder(images)  # [4, 512]
text_features = text_encoder(texts)      # [4, 512]

# Normalize (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´)
image_features = normalize(image_features)
text_features = normalize(text_features)

# Similarity matrix ê³„ì‚°
similarity = image_features @ text_features.T  # [4, 4]
#           text1  text2  text3  text4
# image1  [[0.9,   0.1,   0.05,  0.02],   # img1-text1 ë§¤ì¹­
# image2   [0.1,   0.85,  0.03,  0.08],   # img2-text2 ë§¤ì¹­
# image3   [0.05,  0.02,  0.9,   0.01],   # img3-text3 ë§¤ì¹­
# image4   [0.03,  0.1,   0.02,  0.88]]   # img4-text4 ë§¤ì¹­

```

### Loss ê³„ì‚°
```python
# ëŒ€ê°ì„ (ì˜¬ë°”ë¥¸ ë§¤ì¹­)ì€ ë†’ê²Œ, ë‚˜ë¨¸ì§€ëŠ” ë‚®ê²Œ
labels = [0, 1, 2, 3]  # ëŒ€ê°ì„  ì¸ë±ìŠ¤

# Image-to-Text loss
loss_i2t = CrossEntropyLoss(similarity, labels)

# Text-to-Image loss (ëŒ€ì¹­)
loss_t2i = CrossEntropyLoss(similarity.T, labels)

# Total loss
loss = (loss_i2t + loss_t2i) / 2

```

### íš¨ê³¼
```python
# í…ŒìŠ¤íŠ¸
new_image = "ê³ ì–‘ì´ ì‚¬ì§„"
candidates = ["a cat", "a dog", "a car"]

# Similarity ê³„ì‚°
sims = cosine_similarity(
    vision_encoder(new_image),
    [text_encoder(t) for t in candidates]
)
# ê²°ê³¼: [0.85, 0.2, 0.1]
# â†’ "a cat"ì´ ê°€ìž¥ ë†’ìŒ!
```

ì™œ ê°•ë ¥í•œê°€?
- ëª…ì‹œì ì¸ label ì—†ì´ í•™ìŠµ
- zero-shot classification ê°€ëŠ¥
- ìˆ˜ì–µ ê°œì˜ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥ (ì›¹ í¬ë¡¤ë§)

---

## 6. VLMì˜ 3ê°€ì§€ í•µì‹¬ ì»´í¬ë„ŒíŠ¸
vlmì€ êµ¬ì¡°ì ìœ¼ë¡œ ë³´ë©´ 3ë‹¨ ì—°ê²° êµ¬ì¡°ë‹¤.

### 6.1 Vision Encoder
ëŒ€í‘œ êµ¬ì¡°:
- CNN (ResNet, ConvNeXt)
- ViT (Vision Transformer)

ì¶œë ¥:
- ì´ë¯¸ì§€ ì „ì²´ë¥¼ ëŒ€í‘œí•˜ëŠ” embedding
- ë˜ëŠ” patch-level tokenë“¤

ì¤‘ìš” í¬ì¸íŠ¸:
- ëŒ€ë¶€ë¶„ ì‚¬ì „í•™ìŠµ(pretrained)ëœ ëª¨ë¸
- ì¢…ì¢… freezeë¨ (íŠ¹ížˆ LLM ê²°í•© êµ¬ì¡°ì—ì„œ)

### 6.2 Projection Layer (Bridge)
Vision featureë¥¼ Language ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë‹¤ë¦¬

í•„ìš”í•œ ì´ìœ 
- Vision encoder ì¶œë ¥ ì°¨ì› â‰  LLM ìž…ë ¥ ì°¨ì›
- í‘œí˜„ ë¶„í¬ ìžì²´ë„ ë‹¤ë¦„

ì—­í• 
- image embedding â†’ LLMì´ ì´í•´ ê°€ëŠ¥í•œ token í˜•íƒœë¡œ ë³€í™˜
- linear / MLP / Q-Former ë“± ì‚¬ìš©

ì˜ˆì‹œ
- CLIP: image embedding â†” text embedding ì§ì ‘ ì •ë ¬
- LLaVA/BLIP-2: visual token â†’ language token spaceë¡œ ì‚¬ìƒ

ðŸ“Œ ì´ ë ˆì´ì–´ê°€ cross-modal alignmentì˜ í•µì‹¬ ì§€ì 

### 6.3 LLM (Language Model)
ìµœì¢… ì´í•´, ì¶”ë¡ , ìƒì„± ë‹´ë‹¹

ì—­í• 
- ì‹œê° ì •ë³´ + í…ìŠ¤íŠ¸ instructionì„ ê²°í•©
- reasoning ìˆ˜í–‰
- ìžì—°ì–´ ì‘ë‹µ ìƒì„±

íŠ¹ì§•
- GPT / LLaMA ê³„ì—´
- ì¢…ì¢… freeze + projectionë§Œ í•™ìŠµ
- instruction tuningìœ¼ë¡œ ë©€í‹°ëª¨ë‹¬ ëŠ¥ë ¥ í™•ë³´