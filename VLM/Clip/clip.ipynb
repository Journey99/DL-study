{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de5ff65",
   "metadata": {},
   "source": [
    "# CLIP 모델 구조 상세 분석\n",
    "\n",
    "## 1. CLIP 모델 아키텍처 \n",
    "- vision encoder (resnet or vision transformer 기반) \n",
    "- text encoder (transformer 기반, gpt-like 구조) \n",
    "- projection head (이미지와 텍스트 임베딩을 동일한 차원으로 변환)\n",
    "- contrastive loss (이미지-텍스트 매칭 학습)\n",
    "\n",
    "## 2. CLIP 주요 코드 분석\n",
    "https://github.com/openai/CLIP\n",
    "\n",
    "### (1) Vision Encoder (이미지 인코더)\n",
    "- CLIP의 이미지 인코더는 ResNet 또는 Vision Transformer (ViT)를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cfe368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_b_32\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.model = vit_b_32(pretrained=True)  # 미리 학습된 ViT 사용\n",
    "        self.fc = nn.Linear(768, embed_dim)  # 최종 출력 차원을 512로 변환\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.model(images)  # 이미지 인코딩\n",
    "        x = self.fc(x)  # FC 레이어를 통해 차원 변환\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a96aadf",
   "metadata": {},
   "source": [
    "- vit_b_32(pretrained=True): 사전 학습된 Vision Transformer(ViT) 사용\n",
    "- self.fc = nn.Linear(768, embed_dim): ViT 출력 (768차원) → 512차원 변환\n",
    "- 입력: 이미지 (Tensor)\n",
    "- 출력: 512차원 이미지 임베딩 벡터\n",
    "\n",
    "### (2) Text Encoder (텍스트 인코더)\n",
    "- 텍스트 인코더는 Transformer 기반이고, GPT와 유사한 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.model = RobertaModel.from_pretrained(\"roberta-base\")  # Pretrained Transformer\n",
    "        self.fc = nn.Linear(768, embed_dim)  # 차원 축소\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x = x.last_hidden_state[:, 0, :]  # CLS 토큰 사용\n",
    "        x = self.fc(x)  # FC 레이어\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661639c1",
   "metadata": {},
   "source": [
    "- RobertaModel.from_pretrained(\"roberta-base\"): 사전 학습된 RoBERTa 모델 사용\n",
    "- x.last_hidden_state[:, 0, :]: 문장의 첫 번째 토큰(CLS 토큰) 사용\n",
    "- self.fc = nn.Linear(768, embed_dim): 768차원 → 512차원 변환\n",
    "- 입력 : 토큰화된 텍스트\n",
    "- 출력 : 512차원 텍스트 임베딩\n",
    "\n",
    "### (3) Projection Head (공통 임베딩 공간 매핑)\n",
    "- 이미지와 텍스트 임베딩을 같은 공간으로 변환하기 위해 선형 변환을 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0902b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, embed_dim=512, proj_dim=512):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embed_dim, proj_dim)  # 512 -> 512 (동일 차원 변환)\n",
    "        self.scale = nn.Parameter(torch.ones(1) * 0.07)  # Temperature scaling\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = x / x.norm(dim=-1, keepdim=True)  # L2 정규화\n",
    "        return x * self.scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49bcf5",
   "metadata": {},
   "source": [
    "- nn.Linear(embed_dim, proj_dim): 512차원 → 512차원 변환\n",
    "- x / x.norm(dim=-1, keepdim=True): L2 정규화\n",
    "- self.scale = nn.Parameter(torch.ones(1) * 0.07): Softmax 온도 파라미터\n",
    "\n",
    "### (4) Contrastive Learning (대조 학습)\n",
    "- CLIP은 Contrastive Learning을 사용해서 이미지와 텍스트를 서로 매칭하는 방식으로 학습."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea443b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CLIPLoss(nn.Module):\n",
    "    def forward(self, image_features, text_features):\n",
    "        # 이미지와 텍스트 간 내적 계산\n",
    "        logits = image_features @ text_features.T  # Cosine Similarity\n",
    "        labels = torch.arange(len(logits)).to(logits.device)\n",
    "\n",
    "        # CrossEntropyLoss 적용 (이미지-텍스트 정답 매칭)\n",
    "        loss = (F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels)) / 2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0adf9ee",
   "metadata": {},
   "source": [
    "- logits = image_features @ text_features.T: 이미지와 텍스트 벡터 간 내적(유사도 계산)\n",
    "- labels = torch.arange(len(logits)): 정답 레이블 생성\n",
    "- F.cross_entropy(logits, labels): 정답 텍스트와 가장 유사한 이미지 찾도록 학습\n",
    "\n",
    "### (5) CLIP 전체 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = VisionTransformer(embed_dim)\n",
    "        self.text_encoder = TextEncoder(embed_dim)\n",
    "        self.vision_proj = ProjectionHead(embed_dim)\n",
    "        self.text_proj = ProjectionHead(embed_dim)\n",
    "        self.loss_fn = CLIPLoss()\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        image_features = self.vision_proj(self.vision_encoder(images))\n",
    "        text_features = self.text_proj(self.text_encoder(input_ids, attention_mask))\n",
    "        loss = self.loss_fn(image_features, text_features)\n",
    "        return loss, image_features, text_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
